{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Reproduce Paragraph Vector - Distributed Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Using Stanford's Sentiment Analysis dataset, based on Rotten Tomatoes ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Based on the paper:\n",
    "\n",
    "Le, Q. V., & Mikolov, T. (2014, June). Distributed Representations of Sentences and Documents. In ICML (Vol. 14, pp. 1188-1196).\n",
    "\n",
    "And the workd described in:\n",
    "\n",
    "https://amsterdam.luminis.eu/2017/01/30/implementing-doc2vec/\n",
    "\n",
    "and:\n",
    "\n",
    "https://github.com/wangz10/tensorflow-playground/blob/master/doc2vec.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Install nltk and download punkt package in case it's the first time you run the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Import common constants and functions, including functions that build the dictionary and compute logistic regression to test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from reproduce_par2vec_commons import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load labels from Stanford dataset, including the transformation of numerical values to recover the 5 classes by mapping the positivity probability using the following cut-offs:\n",
    "[0, 0.2], (0.2, 0.4], (0.4, 0.6], (0.6, 0.8], (0.8, 1.0]\n",
    "for very negative, negative, neutral, positive, very positive, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239230\n"
     ]
    }
   ],
   "source": [
    "orig_labels = get_labels()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Builds the dictionary of the words present in the training dataset. It also removes the TOP N most frequent words, where N is defined in the shared constants, and takes just a fied amount of words, discarding also the less frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239230\n",
      "20098\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "dictionary, vocab_size, data, doclens = build_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Compute the window center positions for all documents, sliding the window through the text to obtain the center position that will be used to train the model and shuffle them before using them in the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "892697\n"
     ]
    }
   ],
   "source": [
    "twcp = get_text_window_center_positions(data)\n",
    "print len(twcp)\n",
    "np.random.shuffle(twcp)\n",
    "twcp_train_gen = repeater_shuffler(twcp)\n",
    "del twcp # save some memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The model for Distributed Bag of Words is the simplest of the 2 Par2vec models. In this case the only variables are the embeddings, that are computed by taking advantage of the embedding lookup present in tensorflow, and the softmax weights and biases, that are used in the sampled softmax loss function. Notice that by using this function we are approximating the loss in order to avoid scanning through the entire vocabulary for each gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_training_graph():\n",
    "    # Input data\n",
    "    dataset = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "    labels = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1])\n",
    "    # Weights\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([len(doclens), EMBEDDING_SIZE],\n",
    "                          -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [vocab_size, EMBEDDING_SIZE],\n",
    "            stddev=1.0 / np.sqrt(EMBEDDING_SIZE)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "    # Model\n",
    "    # Look up embeddings for inputs\n",
    "    embed = tf.nn.embedding_lookup(embeddings, dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative\n",
    "    # labels each time\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(\n",
    "            softmax_weights, softmax_biases, labels,\n",
    "            embed, NUM_SAMPLED, vocab_size))\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdagradOptimizer(LEARNING_RATE).minimize(\n",
    "        loss)\n",
    "    # Normalized embeddings (to use cosine similarity later on)\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1,\n",
    "                                 keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    return dataset, labels, softmax_weights, softmax_biases, loss, optimizer, normalized_embeddings, session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "SGD is used to optimize the loss. In this case, each batch is composed by a set of text window center positions. For each twcp, we extract the words surrounding the center included in the window. The prediction task in DBOW is to use the document vector to predict each of these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_batch_single_twcp(twcp, i, batch, labels):\n",
    "    tw_start = twcp - (TEXT_WINDOW_SIZE - 1) // 2\n",
    "    tw_end = twcp + TEXT_WINDOW_SIZE // 2 + 1\n",
    "    docids, wordids = zip(*data[tw_start:tw_end])\n",
    "    batch_slice = slice(i * TEXT_WINDOW_SIZE,\n",
    "                        (i + 1) * TEXT_WINDOW_SIZE)\n",
    "    batch[batch_slice] = docids\n",
    "    labels[batch_slice, 0] = wordids\n",
    "\n",
    "\n",
    "def generate_batch(twcp_gen):\n",
    "    batch = np.ndarray(shape=(BATCH_SIZE,), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(BATCH_SIZE, 1), dtype=np.int32)\n",
    "    for i in range(BATCH_SIZE // TEXT_WINDOW_SIZE):\n",
    "        generate_batch_single_twcp(next(twcp_gen), i, batch, labels)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(optimizer, loss, dataset, labels):\n",
    "    avg_training_loss = 0\n",
    "    for step in range(NUM_STEPS):\n",
    "        batch_data, batch_labels = generate_batch(twcp_train_gen)\n",
    "        _, l = session.run(\n",
    "                [optimizer, loss],\n",
    "                feed_dict={dataset: batch_data, labels: batch_labels})\n",
    "        avg_training_loss += l\n",
    "        if step > 0 and step % REPORT_EVERY_X_STEPS == 0:\n",
    "            avg_training_loss = \\\n",
    "                    avg_training_loss / REPORT_EVERY_X_STEPS\n",
    "            # The average loss is an estimate of the loss over the\n",
    "            # last REPORT_EVERY_X_STEPS batches\n",
    "            print('Average loss at step {:d}: {:.1f}'.format(\n",
    "                    step, avg_training_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We train the embeddings and obtain the computed embeddings and softmax weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 2000: 2.5\n",
      "Average loss at step 4000: 2.3\n",
      "Average loss at step 6000: 2.2\n",
      "Average loss at step 8000: 2.2\n",
      "Average loss at step 10000: 2.2\n"
     ]
    }
   ],
   "source": [
    "dataset, labels, softmax_weights, softmax_biases, loss, optimizer, normalized_embeddings, session = create_training_graph()\n",
    "train(optimizer, loss, dataset, labels)\n",
    "current_embeddings = session.run(normalized_embeddings)\n",
    "current_softmax_weights = session.run(softmax_weights)\n",
    "current_softmax_biases = session.run(softmax_biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For testing we repeat the process, but this time fixing the softmax weights and biases obtained in the training phase. We traing the model again for the test documents in order to compute their embeddings.\n",
    "First we compute the twcp for the test document using the same dictionary that was used during the training.\n",
    "Then the test graph is build. Now the only variable is the embedding, as the softmax weights and biases have been learned during the training.\n",
    "The dataset is prepared by extracting the words from the windows around the twcp and using them as labels for being predict based on the new embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test(doc):\n",
    "    test_data, test_twcp = build_test_twcp(doc, dictionary)\n",
    "\n",
    "    session, test_dataset, test_labels, test_loss, test_normalized_embedding, test_optimizer, test_softmax_biases, test_softmax_weights = create_test_graph(\n",
    "        test_twcp)\n",
    "\n",
    "    for step in range(NUM_STEPS):\n",
    "        test_input = np.ndarray(shape=(TEXT_WINDOW_SIZE*len(test_twcp),), dtype=np.int32)\n",
    "        test_labels_values = np.ndarray(shape=(TEXT_WINDOW_SIZE*len(test_twcp), 1), dtype=np.int32)\n",
    "        i = 0\n",
    "        for twcp in test_twcp:\n",
    "            tw_start = twcp - (TEXT_WINDOW_SIZE - 1) // 2\n",
    "            tw_end = twcp + TEXT_WINDOW_SIZE // 2 + 1\n",
    "            docids, wordids = zip(*test_data[tw_start:tw_end])\n",
    "            batch_slice = slice(i * TEXT_WINDOW_SIZE,\n",
    "                                (i + 1) * TEXT_WINDOW_SIZE)\n",
    "            test_input[batch_slice] = docids\n",
    "            test_labels_values[batch_slice, 0] = wordids\n",
    "            i += 1\n",
    "        _, l = session.run(\n",
    "                [test_optimizer, test_loss],\n",
    "                feed_dict={test_dataset: test_input, test_labels: test_labels_values,\n",
    "                           test_softmax_weights: current_softmax_weights,\n",
    "                           test_softmax_biases: current_softmax_biases})\n",
    "    current_test_embedding = session.run(test_normalized_embedding)\n",
    "    return current_test_embedding\n",
    "\n",
    "\n",
    "def create_test_graph(test_twcps):\n",
    "    # Input data\n",
    "    test_dataset = tf.placeholder(tf.int32, shape=[TEXT_WINDOW_SIZE * len(test_twcps)])\n",
    "    test_labels = tf.placeholder(tf.int32, shape=[TEXT_WINDOW_SIZE * len(test_twcps), 1])\n",
    "    test_softmax_weights = tf.placeholder(tf.float32, shape=[vocab_size, EMBEDDING_SIZE])\n",
    "    test_softmax_biases = tf.placeholder(tf.float32, shape=[vocab_size])\n",
    "    # Weights\n",
    "    test_embedding = tf.Variable(\n",
    "        tf.random_uniform([1, EMBEDDING_SIZE],\n",
    "                          -1.0, 1.0))\n",
    "    # Model\n",
    "    # Look up embeddings for inputs\n",
    "    test_embed = tf.nn.embedding_lookup(test_embedding, test_dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative\n",
    "    # labels each time\n",
    "    test_loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(\n",
    "            test_softmax_weights, test_softmax_biases, test_labels,\n",
    "            test_embed, NUM_SAMPLED, vocab_size))\n",
    "    # Optimizer\n",
    "    test_optimizer = tf.train.AdagradOptimizer(LEARNING_RATE).minimize(\n",
    "        test_loss)\n",
    "    # Normalized embeddings (to use cosine similarity later on)\n",
    "    test_norm = tf.sqrt(tf.reduce_sum(tf.square(test_embedding), 1,\n",
    "                                      keep_dims=True))\n",
    "    test_normalized_embedding = test_embedding / test_norm\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    return session, test_dataset, test_labels, test_loss, test_normalized_embedding, test_optimizer, test_softmax_biases, test_softmax_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to validate the new embeddings obtained in the test, we compute the embeddings twice for the same text and compute the cosine distance, checking that it is around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00589704583848\n"
     ]
    }
   ],
   "source": [
    "test_embedding_1 = test('something cringe-inducing about seeing an American football stadium nuked as pop entertainment')\n",
    "test_embedding_2 = test('something cringe-inducing about seeing an American football stadium nuked as pop entertainment')\n",
    "distance = spatial.distance.cosine(test_embedding_1, test_embedding_2)\n",
    "print distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally we compute a Logistic regression taking the embeddings as inputs for the phrases of the dataset and the sentiment labels computed at the beginning. The accuracy obtained must be around 48,7 or above, which was the value obtained by Mikolov in the original Paragraph Vector paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.498244\n"
     ]
    }
   ],
   "source": [
    "test_logistic_regression(current_embeddings, orig_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
