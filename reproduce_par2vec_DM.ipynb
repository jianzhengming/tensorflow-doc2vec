{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Reproduce Paragraph Vector - Distributed Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Using Stanford's Sentiment Analysis dataset, based on Rotten Tomatoes ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Based on the paper:\n",
    "\n",
    "Le, Q. V., & Mikolov, T. (2014, June). Distributed Representations of Sentences and Documents. In ICML (Vol. 14, pp. 1188-1196).\n",
    "\n",
    "And the workd described in:\n",
    "\n",
    "https://amsterdam.luminis.eu/2017/01/30/implementing-doc2vec/\n",
    "\n",
    "and:\n",
    "\n",
    "https://github.com/wangz10/tensorflow-playground/blob/master/doc2vec.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Install nltk and download punkt package in case it's the first time you run the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.2.2.tar.gz (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 341kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/envs/python2/lib/python2.7/site-packages (from nltk)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/42/b5/27/718985cd9719e8a44a405d264d98214c7a607fb65f3a006f28\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.2.2\n",
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> punkt\n",
      "    Downloading package punkt to /home/jovyan/nltk_data...\n",
      "      Unzipping tokenizers/punkt.zip.\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "# download punkt package\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Import common constants and functions, including functions that build the dictionary and compute logistic regression to test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named utils",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-71b120e488ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mreproduce_par2vec_commons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jovyan/work/reproduce_par2vec_commons.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mTEXT_WINDOW_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named utils"
     ]
    }
   ],
   "source": [
    "from reproduce_par2vec_commons import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load labels from Stanford dataset, including the transformation of numerical values to recover the 5 classes by mapping the positivity probability using the following cut-offs:\n",
    "[0, 0.2], (0.2, 0.4], (0.4, 0.6], (0.6, 0.8], (0.8, 1.0]\n",
    "for very negative, negative, neutral, positive, very positive, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "orig_labels = get_labels()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Builds the dictionary of the words present in the training dataset. It also removes the TOP N most frequent words, where N is defined in the shared constants, and takes just a fied amount of words, discarding also the less frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10433\n",
      "10335\n"
     ]
    }
   ],
   "source": [
    "dictionary, vocab_size, data, doclens = build_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Compute the window center positions for all documents, sliding the window through the text to obtain the center position that will be used to train the model and shuffle them before using them in the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53398\n"
     ]
    }
   ],
   "source": [
    "twcp = get_text_window_center_positions(data)\n",
    "print len(twcp)\n",
    "np.random.shuffle(twcp)\n",
    "twcp_train_gen = repeater_shuffler(twcp)\n",
    "del twcp  # save some memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The Paragraph to vector, in its distributed memory version, combines the embedding of the vector with the embeddings of the word of the window that encloses the word to predict. In the present version, the combination of the vectors is done by concatenating them all together. \n",
    "In the DM model, we introduce embeddings for the documents, for the words and also variables for the softmax weights and biases used in the prediction of the center word. In DM, the main goal is to predict the center word of each window based on the rest of words and the document embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_training_graph():\n",
    "    # Input data\n",
    "    dataset = tf.placeholder(tf.int32, shape=[BATCH_SIZE, TEXT_WINDOW_SIZE])\n",
    "    labels = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1])\n",
    "    # Variables.\n",
    "    # embeddings for words, W in paper\n",
    "    word_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocab_size, EMBEDDING_SIZE], -1.0, 1.0))\n",
    "    # embedding for documents (can be sentences or paragraph), D in paper\n",
    "    doc_embeddings = tf.Variable(\n",
    "        tf.random_uniform([len(doclens), EMBEDDING_SIZE], -1.0, 1.0))\n",
    "    combined_embed_vector_length = EMBEDDING_SIZE * TEXT_WINDOW_SIZE\n",
    "    # softmax weights, W and D vectors should be concatenated before applying softmax\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocab_size, combined_embed_vector_length],\n",
    "                            stddev=1.0 / np.math.sqrt(combined_embed_vector_length)))\n",
    "    # softmax biases\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    # shape: (batch_size, embeddings_size)\n",
    "    embed = []  # collect embedding matrices with shape=(batch_size, embedding_size)\n",
    "    for j in range(TEXT_WINDOW_SIZE - 1):\n",
    "        embed_w = tf.nn.embedding_lookup(word_embeddings, dataset[:, j])\n",
    "        embed.append(embed_w)\n",
    "    embed_d = tf.nn.embedding_lookup(doc_embeddings, dataset[:, TEXT_WINDOW_SIZE - 1])\n",
    "    embed.append(embed_d)\n",
    "    # concat word and doc vectors\n",
    "    embed = tf.concat(embed, 1)\n",
    "    # Compute the softmax loss, using a sample of the negative\n",
    "    # labels each time\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(\n",
    "            softmax_weights, softmax_biases, labels,\n",
    "            embed, NUM_SAMPLED, vocab_size))\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdagradOptimizer(LEARNING_RATE).minimize(\n",
    "        loss)\n",
    "    # We use the cosine distance:\n",
    "    norm_w = tf.sqrt(tf.reduce_sum(tf.square(word_embeddings), 1, keep_dims=True))\n",
    "    normalized_word_embeddings = word_embeddings / norm_w\n",
    "    norm_d = tf.sqrt(tf.reduce_sum(tf.square(doc_embeddings), 1, keep_dims=True))\n",
    "    normalized_doc_embeddings = doc_embeddings / norm_d\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    return optimizer, loss, dataset, labels,\\\n",
    "           normalized_word_embeddings, \\\n",
    "           normalized_doc_embeddings, \\\n",
    "           session, softmax_weights, softmax_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "SGD is used to optimize the loss. In this case, each batch is composed by a set of text window center positions. For each twcp, we create a list of the surrounding words and we concatenate the embedding of the document to this list. The label to predict is the central word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_batch_single_twcp(twcp, i, batch, labels):\n",
    "    tw_start = twcp - (TEXT_WINDOW_SIZE - 1) // 2\n",
    "    tw_end = twcp + TEXT_WINDOW_SIZE // 2 + 1\n",
    "    docids, wordids = zip(*data[tw_start:tw_end])\n",
    "\n",
    "    wordids_list = list(wordids)\n",
    "    twcp_index = (TEXT_WINDOW_SIZE - 1) // 2\n",
    "    twcp_docid = data[twcp][0]\n",
    "    twcp_wordid = data[twcp][1]\n",
    "    del wordids_list[twcp_index]\n",
    "    wordids_list.append(twcp_docid)\n",
    "\n",
    "    batch[i] = wordids_list\n",
    "    labels[i] = twcp_wordid\n",
    "\n",
    "\n",
    "def generate_batch(twcp_gen):\n",
    "    batch = np.ndarray(shape=(BATCH_SIZE, TEXT_WINDOW_SIZE), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(BATCH_SIZE, 1), dtype=np.int32)\n",
    "    for i in range(BATCH_SIZE):\n",
    "        generate_batch_single_twcp(next(twcp_gen), i, batch, labels)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(optimizer, loss, dataset, labels):\n",
    "    avg_training_loss = 0\n",
    "    for step in range(NUM_STEPS):\n",
    "        batch_data, batch_labels = generate_batch(twcp_train_gen)\n",
    "        _, l = session.run(\n",
    "            [optimizer, loss],\n",
    "            feed_dict={dataset: batch_data, labels: batch_labels})\n",
    "        avg_training_loss += l\n",
    "        if step > 0 and step % REPORT_EVERY_X_STEPS == 0:\n",
    "            avg_training_loss = \\\n",
    "                avg_training_loss / REPORT_EVERY_X_STEPS\n",
    "            # The average loss is an estimate of the loss over the\n",
    "            # last REPORT_EVERY_X_STEPS batches\n",
    "            print('Average loss at step {:d}: {:.1f}'.format(\n",
    "                step, avg_training_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We train the embeddings and obtain the computed embeddings and softmax weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 2000: 1.3\n",
      "Average loss at step 4000: 0.4\n",
      "Average loss at step 6000: 0.2\n",
      "Average loss at step 8000: 0.1\n",
      "Average loss at step 10000: 0.1\n"
     ]
    }
   ],
   "source": [
    "optimizer, loss, dataset, labels, word_embeddings, doc_embeddings, session, softmax_weights, softmax_biases = create_training_graph()\n",
    "train(optimizer, loss, dataset, labels)\n",
    "current_embeddings = session.run(doc_embeddings)\n",
    "current_word_embeddings = session.run(word_embeddings)\n",
    "current_softmax_weights = session.run(softmax_weights)\n",
    "current_softmax_biases = session.run(softmax_biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For testing we repeat the process, but this time fixing the word embeddings and the softmax weights and biases obtained in the training phase. We traing the model again for the test documents in order to compute their embeddings.\n",
    "First we compute the twcp for the test document using the same dictionary that was used during the training.\n",
    "Then the test graph is build. Now the only variable is the document embedding, as the softmax weights and biases and word embeddings have been learned during the training.\n",
    "The dataset is prepared by extracting the words from the windows around the twcp, together with the document id, and using the center word id as the label to predict based on the new embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test(doc, train_word_embeddings, train_softmax_weights, train_softmax_biases):\n",
    "    test_data, test_twcp = build_test_twcp(doc, dictionary)\n",
    "    # Input data\n",
    "    combined_embed_vector_length = EMBEDDING_SIZE * TEXT_WINDOW_SIZE\n",
    "    test_dataset = tf.placeholder(tf.int32, shape=[len(test_twcp), TEXT_WINDOW_SIZE])\n",
    "    test_labels = tf.placeholder(tf.int32, shape=[len(test_twcp), 1])\n",
    "    test_softmax_weights = tf.placeholder(tf.float32, shape=[vocab_size, combined_embed_vector_length])\n",
    "    test_softmax_biases = tf.placeholder(tf.float32, shape=[vocab_size])\n",
    "    test_word_embeddings = tf.placeholder(tf.float32, shape=[vocab_size, EMBEDDING_SIZE])\n",
    "    # Variables.\n",
    "    # embedding for documents (can be sentences or paragraph), D in paper\n",
    "    test_doc_embeddings = tf.Variable(\n",
    "        tf.random_uniform([1, EMBEDDING_SIZE], -1.0, 1.0))\n",
    "\n",
    "    # Look up embeddings for inputs.\n",
    "    # shape: (batch_size, embeddings_size)\n",
    "    test_embed = []  # collect embedding matrices with shape=(batch_size, embedding_size)\n",
    "    for j in range(TEXT_WINDOW_SIZE - 1):\n",
    "        test_embed_w = tf.gather(test_word_embeddings, test_dataset[:,j])\n",
    "        test_embed.append(test_embed_w)\n",
    "    test_embed_d = tf.nn.embedding_lookup(test_doc_embeddings, test_dataset[:, TEXT_WINDOW_SIZE - 1])\n",
    "    test_embed.append(test_embed_d)\n",
    "    # concat word and doc vectors\n",
    "    test_embed = tf.concat(test_embed, 1)\n",
    "    # Compute the softmax loss, using a sample of the negative\n",
    "    # labels each time\n",
    "    test_loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(\n",
    "            test_softmax_weights, test_softmax_biases, test_labels,\n",
    "            test_embed, NUM_SAMPLED, vocab_size))\n",
    "    # Optimizer\n",
    "    test_optimizer = tf.train.AdagradOptimizer(LEARNING_RATE).minimize(\n",
    "        test_loss)\n",
    "    # We use the cosine distance:\n",
    "    test_norm_d = tf.sqrt(tf.reduce_sum(tf.square(test_doc_embeddings), 1, keep_dims=True))\n",
    "    test_normalized_doc_embeddings = test_doc_embeddings / test_norm_d\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(NUM_STEPS):\n",
    "        test_input = np.ndarray(shape=(len(test_twcp), TEXT_WINDOW_SIZE), dtype=np.int32)\n",
    "        labels_values = np.ndarray(shape=(len(test_twcp), 1), dtype=np.int32)\n",
    "        i = 0\n",
    "        for twcp in test_twcp:\n",
    "            tw_start = twcp - (TEXT_WINDOW_SIZE - 1) // 2\n",
    "            tw_end = twcp + TEXT_WINDOW_SIZE // 2 + 1\n",
    "            docids, wordids = zip(*test_data[tw_start:tw_end])\n",
    "\n",
    "            wordids_list = list(wordids)\n",
    "            twcp_index = (TEXT_WINDOW_SIZE - 1) // 2\n",
    "            twcp_docid = test_data[twcp][0]\n",
    "            twcp_wordid = test_data[twcp][1]\n",
    "            del wordids_list[twcp_index]\n",
    "            wordids_list.append(twcp_docid)\n",
    "\n",
    "            test_input[i] = wordids_list\n",
    "            labels_values[i] = twcp_wordid\n",
    "            i += 1\n",
    "        _, l = session.run(\n",
    "            [test_optimizer, test_loss],\n",
    "            feed_dict={test_dataset: test_input, test_labels: labels_values,\n",
    "                       test_word_embeddings: train_word_embeddings,\n",
    "                       test_softmax_weights: train_softmax_weights,\n",
    "                       test_softmax_biases: train_softmax_biases\n",
    "                       })\n",
    "    current_test_embedding = session.run(test_normalized_doc_embeddings)\n",
    "    return current_test_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to validate the new embeddings obtained in the test, we compute the embeddings twice for the same text and compute the cosine distance, checking that it is around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024357977433\n"
     ]
    }
   ],
   "source": [
    "test_embedding_1 = test('something cringe-inducing about seeing an American football stadium nuked as pop entertainment',\n",
    "                        current_word_embeddings, current_softmax_weights, current_softmax_biases)\n",
    "test_embedding_2 = test('something cringe-inducing about seeing an American football stadium nuked as pop entertainment',\n",
    "                        current_word_embeddings, current_softmax_weights, current_softmax_biases)\n",
    "distance = spatial.distance.cosine(test_embedding_1, test_embedding_2)\n",
    "print distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally we compute a Logistic regression taking the embeddings as inputs for the phrases of the dataset and the sentiment labels computed at the beginning. The accuracy obtained must be around 48,7 or above, which was the value obtained by Mikolov in the original Paragraph Vector paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5235\n"
     ]
    }
   ],
   "source": [
    "test_logistic_regression(current_embeddings, orig_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
